{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6ZipXWjNQQW"
      },
      "outputs": [],
      "source": [
        "from pandas_datareader import data\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import urllib.request, json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnHmf9VTZSel"
      },
      "outputs": [],
      "source": [
        "data_source = 'kaggle' # alphavantage or kaggle\n",
        "\n",
        "if data_source == 'alphavantage':\n",
        "    # ====================== Loading Data from Alpha Vantage ==================================\n",
        "\n",
        "    api_key = '104SAFLO5FU4FCKF'\n",
        "\n",
        "    # American Airlines stock market prices\n",
        "    ticker = \"AAL\"\n",
        "\n",
        "    # JSON file with all the stock market data for AAL from the last 20 years\n",
        "    url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(ticker,api_key)\n",
        "\n",
        "    # Save data to this file\n",
        "    file_to_save = 'stock_market_data-%s.csv'%ticker\n",
        "\n",
        "    # If you haven't already saved data,\n",
        "    # Go ahead and grab the data from the url\n",
        "    # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
        "    if not os.path.exists(file_to_save):\n",
        "        with urllib.request.urlopen(url_string) as url:\n",
        "            data = json.loads(url.read().decode())\n",
        "            # extract stock market data\n",
        "            data = data['Time Series (Daily)']\n",
        "            df = pd.DataFrame(columns=['Date','Low','High','Close','Open'])\n",
        "            for k,v in data.items():\n",
        "                date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
        "                data_row = [date.date(),float(v['3. low']),float(v['2. high']),\n",
        "                            float(v['4. close']),float(v['1. open'])]\n",
        "                df.loc[-1,:] = data_row\n",
        "                df.index = df.index + 1\n",
        "        print('Data saved to : %s'%file_to_save)\n",
        "        df.to_csv(file_to_save)\n",
        "\n",
        "    # If the data is already there, just load it from the CSV\n",
        "    else:\n",
        "        print('File already exists. Loading data from CSV')\n",
        "        df = pd.read_csv(file_to_save)\n",
        "\n",
        "else:\n",
        "\n",
        "    # ====================== Loading Data from Kaggle ==================================\n",
        "    # You will be using HP's data. Feel free to experiment with other data.\n",
        "    # But while doing so, be careful to have a large enough dataset and also pay attention to the data normalization\n",
        "    df = pd.read_csv(os.path.join('/content/drive/MyDrive/Stocks','hpq.us.txt'),delimiter=',',usecols=['Date','Open','High','Low','Close'])\n",
        "    print('Loaded data from the Kaggle repository')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfSt0piE5-An"
      },
      "outputs": [],
      "source": [
        "# Sort DataFrame by date\n",
        "df = df.sort_values('Date')\n",
        "\n",
        "# Double check the result\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbGMu6y9Q_3M"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (18,9))\n",
        "plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)\n",
        "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\n",
        "plt.xlabel('Date',fontsize=18)\n",
        "plt.ylabel('Mid Price',fontsize=18)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "427HJoCL6DbJ"
      },
      "outputs": [],
      "source": [
        "# First calculate the mid prices from the highest and lowest\n",
        "high_prices = df.loc[:, 'High'].to_numpy()\n",
        "low_prices = df.loc[:, 'Low'].to_numpy()\n",
        "mid_prices = (high_prices + low_prices) / 2.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EprPtOpx1J0o"
      },
      "outputs": [],
      "source": [
        "train_data = mid_prices[:11000]\n",
        "test_data = mid_prices[11000:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yYsSCb_1L3t"
      },
      "outputs": [],
      "source": [
        "# Scale the data to be between 0 and 1\n",
        "# When scaling remember! You normalize both test and train data with respect to training data\n",
        "# Because you are not supposed to have access to test data\n",
        "scaler = MinMaxScaler()\n",
        "train_data = train_data.reshape(-1,1)\n",
        "test_data = test_data.reshape(-1,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L_r09Ss1Nu6"
      },
      "outputs": [],
      "source": [
        "# Train the Scaler with training data and smooth data\n",
        "smoothing_window_size = 2500\n",
        "for di in range(0,10000,smoothing_window_size):\n",
        "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
        "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
        "\n",
        "# You normalize the last bit of remaining data\n",
        "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
        "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDogVX_M1Pwx"
      },
      "outputs": [],
      "source": [
        "# Reshape both train and test data\n",
        "train_data = train_data.reshape(-1)\n",
        "\n",
        "# Normalize test data\n",
        "test_data = scaler.transform(test_data).reshape(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzTHMlny1R13"
      },
      "outputs": [],
      "source": [
        "# Now perform exponential moving average smoothing\n",
        "# So the data will have a smoother curve than the original ragged data\n",
        "EMA = 0.0\n",
        "gamma = 0.1\n",
        "for ti in range(11000):\n",
        "  EMA = gamma*train_data[ti] + (1-gamma)*EMA\n",
        "  train_data[ti] = EMA\n",
        "\n",
        "# Used for visualization and test purposes\n",
        "all_mid_data = np.concatenate([train_data,test_data],axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mhjAXfe1Tt_"
      },
      "outputs": [],
      "source": [
        "window_size = 100\n",
        "N = train_data.size\n",
        "std_avg_predictions = []\n",
        "std_avg_x = []\n",
        "mse_errors = []\n",
        "\n",
        "for pred_idx in range(window_size,N):\n",
        "\n",
        "    if pred_idx >= N:\n",
        "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
        "    else:\n",
        "        date = df.loc[pred_idx,'Date']\n",
        "\n",
        "    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n",
        "    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n",
        "    std_avg_x.append(date)\n",
        "\n",
        "print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xENb2dTu1WIx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (18,9))\n",
        "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
        "plt.plot(range(window_size,N),std_avg_predictions,color='orange',label='Prediction')\n",
        "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Mid Price')\n",
        "plt.legend(fontsize=18)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrDbVQoL1cbk"
      },
      "outputs": [],
      "source": [
        "window_size = 100\n",
        "N = train_data.size\n",
        "\n",
        "run_avg_predictions = []\n",
        "run_avg_x = []\n",
        "\n",
        "mse_errors = []\n",
        "\n",
        "running_mean = 0.0\n",
        "run_avg_predictions.append(running_mean)\n",
        "\n",
        "decay = 0.5\n",
        "\n",
        "for pred_idx in range(1,N):\n",
        "\n",
        "    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n",
        "    run_avg_predictions.append(running_mean)\n",
        "    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n",
        "    run_avg_x.append(date)\n",
        "\n",
        "print('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74Ix0HDS1enw"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize = (18,9))\n",
        "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
        "plt.plot(range(0,N),run_avg_predictions,color='orange', label='Prediction')\n",
        "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Mid Price')\n",
        "plt.legend(fontsize=18)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xkybzLA1iOq"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataGeneratorSeq(object):\n",
        "\n",
        "    def __init__(self,prices,batch_size,num_unroll):\n",
        "        self._prices = prices\n",
        "        self._prices_length = len(self._prices) - num_unroll\n",
        "        self._batch_size = batch_size\n",
        "        self._num_unroll = num_unroll\n",
        "        self._segments = self._prices_length //self._batch_size\n",
        "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
        "\n",
        "    def next_batch(self):\n",
        "\n",
        "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
        "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
        "\n",
        "        for b in range(self._batch_size):\n",
        "            if self._cursor[b]+1>=self._prices_length:\n",
        "                #self._cursor[b] = b * self._segments\n",
        "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
        "\n",
        "            batch_data[b] = self._prices[self._cursor[b]]\n",
        "            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n",
        "\n",
        "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
        "\n",
        "        return batch_data,batch_labels\n",
        "\n",
        "    def unroll_batches(self):\n",
        "\n",
        "        unroll_data,unroll_labels = [],[]\n",
        "        init_data, init_label = None,None\n",
        "        for ui in range(self._num_unroll):\n",
        "\n",
        "            data, labels = self.next_batch()\n",
        "\n",
        "            unroll_data.append(data)\n",
        "            unroll_labels.append(labels)\n",
        "\n",
        "        return unroll_data, unroll_labels\n",
        "\n",
        "    def reset_indices(self):\n",
        "        for b in range(self._batch_size):\n",
        "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n",
        "\n",
        "\n",
        "\n",
        "dg = DataGeneratorSeq(train_data,5,5)\n",
        "u_data, u_labels = dg.unroll_batches()\n",
        "\n",
        "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):\n",
        "    print('\\n\\nUnrolled index %d'%ui)\n",
        "    dat_ind = dat\n",
        "    lbl_ind = lbl\n",
        "    print('\\tInputs: ',dat )\n",
        "    print('\\n\\tOutput:',lbl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BASK81v11lwp"
      },
      "outputs": [],
      "source": [
        "D = 1 # Dimensionality of the data. Since your data is 1-D this would be 1\n",
        "num_unrollings = 50 # Number of time steps you look into the future.\n",
        "batch_size = 500 # Number of samples in a batch\n",
        "num_nodes = [200,200,150] # Number of hidden nodes in each layer of the deep LSTM stack we're using\n",
        "n_layers = len(num_nodes) # number of layers\n",
        "dropout = 0.2 # dropout amount\n",
        "\n",
        "tf.keras.backend.clear_session() # This is important in case you run this multiple times\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1A2Tydr1oXJ"
      },
      "outputs": [],
      "source": [
        "train_inputs, train_outputs = [], []\n",
        "\n",
        "# Simulate input shapes (placeholders are gone in TF 2.x)\n",
        "# You can append tensors or numpy arrays here\n",
        "for ui in range(num_unrollings):\n",
        "    # Use dummy tensors or numpy arrays if needed\n",
        "    dummy_input = tf.zeros([batch_size, D])  # shape like your original placeholder\n",
        "    dummy_output = tf.zeros([batch_size, 1]) # shape like your original placeholder\n",
        "    train_inputs.append(dummy_input)\n",
        "    train_outputs.append(dummy_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR3m3O5f3Y4L"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM layers with dropout support\n",
        "lstm_layers = [\n",
        "    tf.keras.layers.LSTM(units=num_nodes[li],\n",
        "                         kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
        "                         return_sequences=True if li < n_layers - 1 else False,  # For multi-layer RNN\n",
        "                         dropout=dropout,  # Dropout for the input\n",
        "                         recurrent_dropout=dropout)  # Dropout for the recurrent state\n",
        "    for li in range(n_layers)\n",
        "]\n",
        "\n",
        "# Stack the LSTM layers into a Sequential model or use a Functional API model\n",
        "model = tf.keras.Sequential(lstm_layers)\n",
        "\n",
        "# Define the output layer (fully connected layer)\n",
        "w = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=[num_nodes[-1], 1]))\n",
        "b = tf.Variable(tf.random.uniform([1], -0.1, 0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgPBWaHVOaEp"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM cells for the multi-layer RNN\n",
        "drop_multi_cell = tf.keras.layers.StackedRNNCells([tf.keras.layers.LSTMCell(num_nodes[i]) for i in range(n_layers)])\n",
        "\n",
        "# Create cell state and hidden state variables to maintain the state of the LSTM\n",
        "initial_state = []\n",
        "for li in range(n_layers):\n",
        "    h = tf.zeros([batch_size, num_nodes[li]])\n",
        "    c = tf.zeros([batch_size, num_nodes[li]])\n",
        "    initial_state.append([c, h])  # tf.keras.layers.LSTMCell expects [c, h] tuple\n",
        "\n",
        "# Prepare inputs in [batch_size, time_steps, input_dim] format\n",
        "all_inputs = tf.transpose(tf.stack(train_inputs, axis=0), perm=[1, 0, 2])\n",
        "\n",
        "# Build multi-layer RNN (no time_major here)\n",
        "rnn_layer = tf.keras.layers.RNN(drop_multi_cell, return_sequences=True, return_state=True)\n",
        "\n",
        "# Run the RNN\n",
        "all_lstm_outputs, *state = rnn_layer(all_inputs, initial_state=initial_state)\n",
        "\n",
        "# all_lstm_outputs shape: [batch_size, time_steps, num_nodes[-1]]\n",
        "# Reshape to match [batch_size * num_unrollings, num_nodes[-1]]\n",
        "all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size * num_unrollings, num_nodes[-1]])\n",
        "\n",
        "# Linear output layer\n",
        "all_outputs = tf.matmul(all_lstm_outputs, w) + b\n",
        "\n",
        "# Split into [batch_size, output_dim] tensors\n",
        "split_outputs = tf.split(all_outputs, num_unrollings, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHR5B_sd3bbU"
      },
      "outputs": [],
      "source": [
        "print('Defining training Loss')\n",
        "\n",
        "# Calculate loss as mean squared error across all unrolling steps\n",
        "loss = 0.0\n",
        "for ui in range(num_unrollings):\n",
        "    loss += tf.reduce_mean(0.5 * (split_outputs[ui] - train_outputs[ui]) ** 2)\n",
        "\n",
        "print('Learning rate decay operations')\n",
        "\n",
        "# Learning rate scheduling\n",
        "global_step = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
        "initial_learning_rate = 0.01  # example value, replace as needed\n",
        "min_learning_rate = 0.0001    # example value\n",
        "\n",
        "# Define decaying learning rate with minimum clip\n",
        "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=1,\n",
        "    decay_rate=0.5,\n",
        "    staircase=True\n",
        ")\n",
        "learning_rate = lambda: tf.maximum(learning_rate(global_step), min_learning_rate)\n",
        "\n",
        "# Optimizer\n",
        "print('TF Optimization operations')\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training step function\n",
        "@tf.function\n",
        "def train_step():\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Recompute loss inside the tape\n",
        "        loss_value = 0.0\n",
        "        for ui in range(num_unrollings):\n",
        "            loss_value += tf.reduce_mean(0.5 * (split_outputs[ui] - train_outputs[ui]) ** 2)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    global_step.assign_add(1)\n",
        "    return loss_value\n",
        "\n",
        "print('\\tAll done')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hOdnztMSmn2"
      },
      "outputs": [],
      "source": [
        "print('Defining prediction related TF functions')\n",
        "\n",
        "# Input placeholder is replaced by a tf.function argument or tensor\n",
        "sample_inputs = tf.keras.Input(shape=(D,), batch_size=1)\n",
        "\n",
        "# Maintaining LSTM state for prediction stage\n",
        "sample_c, sample_h, initial_sample_state = [], [], []\n",
        "for li in range(n_layers):\n",
        "    sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
        "    sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
        "    initial_sample_state.append([sample_c[li], sample_h[li]])\n",
        "\n",
        "# LSTM cell\n",
        "cells = [tf.keras.layers.LSTMCell(num_nodes[li]) for li in range(n_layers)]\n",
        "multi_cell = tf.keras.layers.StackedRNNCells(cells)\n",
        "rnn_layer = tf.keras.layers.RNN(multi_cell, return_state=True, return_sequences=True)\n",
        "\n",
        "# Function to reset LSTM state\n",
        "@tf.function\n",
        "def reset_sample_states():\n",
        "    for li in range(n_layers):\n",
        "        sample_c[li].assign(tf.zeros_like(sample_c[li]))\n",
        "        sample_h[li].assign(tf.zeros_like(sample_h[li]))\n",
        "\n",
        "# Prediction function\n",
        "@tf.function\n",
        "def predict(sample_input):\n",
        "    input_expanded = tf.expand_dims(tf.expand_dims(sample_input, axis=0), axis=0)  # [1, 1, D]\n",
        "    initial_state = [(sample_c[li], sample_h[li]) for li in range(n_layers)]\n",
        "    output, *new_states = rnn_layer(input_expanded, initial_state=initial_state)\n",
        "\n",
        "    # Update states\n",
        "    for li in range(n_layers):\n",
        "        sample_c[li].assign(new_states[li][0])\n",
        "        sample_h[li].assign(new_states[li][1])\n",
        "\n",
        "    # Fully connected layer to get prediction\n",
        "    output_reshaped = tf.reshape(output, [1, -1])  # [1, num_nodes[-1]]\n",
        "    sample_prediction = tf.matmul(output_reshaped, w) + b\n",
        "    return sample_prediction\n",
        "\n",
        "print('\\tAll done')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xt0iM6FISqAi"
      },
      "outputs": [],
      "source": [
        "\n",
        "epochs = 30\n",
        "valid_summary = 1  # Interval for test predictions\n",
        "n_predict_once = 50\n",
        "\n",
        "train_seq_length = train_data.size\n",
        "train_mse_ot = []\n",
        "test_mse_ot = []\n",
        "predictions_over_time = []\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "initial_learning_rate = 0.0001\n",
        "min_learning_rate = 0.000001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.5,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Loss function\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Data generator\n",
        "data_gen = DataGeneratorSeq(train_data, batch_size, num_unrollings)\n",
        "test_points_seq = np.arange(11000, 12000, 50).tolist()\n",
        "x_axis_seq = []\n",
        "\n",
        "print(\"Initialized\")\n",
        "\n",
        "loss_nondecrease_count = 0\n",
        "loss_nondecrease_threshold = 2\n",
        "\n",
        "for ep in range(epochs):\n",
        "    print(f\"\\nEpoch {ep+1}/{epochs}\")\n",
        "    average_loss = 0\n",
        "\n",
        "    # === Training ===\n",
        "    for step in range(train_seq_length // batch_size):\n",
        "        u_data, u_labels = data_gen.unroll_batches()\n",
        "\n",
        "        # Stack inputs and labels\n",
        "        batch_inputs = np.stack(u_data, axis=1).reshape(batch_size, num_unrollings, 1)\n",
        "        batch_labels = np.stack(u_labels, axis=1).reshape(batch_size, num_unrollings, 1)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(batch_inputs, training=True)\n",
        "            loss = mse_loss(batch_labels[:, -1], predictions)  # match target time step\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        average_loss += loss.numpy()\n",
        "\n",
        "    # === Validation ===\n",
        "    if (ep + 1) % valid_summary == 0:\n",
        "        average_loss /= (train_seq_length // batch_size)\n",
        "        print(f\"Average training loss: {average_loss:.6f}\")\n",
        "        train_mse_ot.append(average_loss)\n",
        "\n",
        "        predictions_seq = []\n",
        "        mse_test_loss_seq = []\n",
        "\n",
        "        for w_i in test_points_seq:\n",
        "            mse_test_loss = 0.0\n",
        "            our_predictions = []\n",
        "            x_axis = []\n",
        "\n",
        "            # Priming the model with past values\n",
        "            history_input = all_mid_data[w_i - num_unrollings + 1: w_i].reshape(1, num_unrollings - 1, 1)\n",
        "            for _ in range(num_unrollings - 1):\n",
        "                _ = model(history_input, training=False)\n",
        "\n",
        "            # First input to predict\n",
        "            current_price = all_mid_data[w_i - 1].reshape(1, 1, 1)\n",
        "\n",
        "            for pred_i in range(n_predict_once):\n",
        "                pred = model(current_price, training=False)\n",
        "                pred_val = pred.numpy().flatten()[0]\n",
        "                our_predictions.append(pred_val)\n",
        "                current_price = np.array(pred_val).reshape(1, 1, 1)\n",
        "\n",
        "                if (ep + 1) - valid_summary == 0:\n",
        "                    x_axis.append(w_i + pred_i)\n",
        "\n",
        "                true_val = all_mid_data[w_i + pred_i]\n",
        "                mse_test_loss += 0.5 * (pred_val - true_val) ** 2\n",
        "\n",
        "            predictions_seq.append(our_predictions)\n",
        "            mse_test_loss /= n_predict_once\n",
        "            mse_test_loss_seq.append(mse_test_loss)\n",
        "\n",
        "            if (ep + 1) - valid_summary == 0:\n",
        "                x_axis_seq.append(x_axis)\n",
        "\n",
        "        current_test_mse = np.mean(mse_test_loss_seq)\n",
        "\n",
        "        if len(test_mse_ot) > 0 and current_test_mse > min(test_mse_ot):\n",
        "            loss_nondecrease_count += 1\n",
        "        else:\n",
        "            loss_nondecrease_count = 0\n",
        "\n",
        "        if loss_nondecrease_count > loss_nondecrease_threshold:\n",
        "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "                optimizer.learning_rate.numpy() * 0.5,\n",
        "                decay_steps=1000,\n",
        "                decay_rate=0.5,\n",
        "                staircase=True\n",
        "            )\n",
        "            optimizer.learning_rate = lr_schedule\n",
        "            loss_nondecrease_count = 0\n",
        "            print(\"\\tDecreasing learning rate by 0.5\")\n",
        "\n",
        "        test_mse_ot.append(current_test_mse)\n",
        "        print(f\"\\tTest MSE: {current_test_mse:.5f}\")\n",
        "        predictions_over_time.append(predictions_seq)\n",
        "        print(\"\\tFinished Predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvqjRll8SuCY"
      },
      "outputs": [],
      "source": [
        "best_prediction_epoch =  28 # replace this with the epoch that you got the best results when running the plotting code\n",
        "\n",
        "plt.figure(figsize = (18,18))\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
        "\n",
        "# Plotting how the predictions change over time\n",
        "# Plot older predictions with low alpha and newer predictions with high alpha\n",
        "start_alpha = 0.25\n",
        "alpha  = np.arange(start_alpha,1.1,(1.0-start_alpha)/len(predictions_over_time[::3]))\n",
        "for p_i,p in enumerate(predictions_over_time[::3]):\n",
        "    for xval,yval in zip(x_axis_seq,p):\n",
        "        plt.plot(xval,yval,color='r',alpha=alpha[p_i])\n",
        "\n",
        "plt.title('Evolution of Test Predictions Over Time',fontsize=18)\n",
        "plt.xlabel('Date',fontsize=18)\n",
        "plt.ylabel('Mid Price',fontsize=18)\n",
        "plt.xlim(11000,12500)\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "# Predicting the best test prediction you got\n",
        "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
        "for xval,yval in zip(x_axis_seq,predictions_over_time[best_prediction_epoch]):\n",
        "    plt.plot(xval,yval,color='r')\n",
        "\n",
        "plt.title('Best Test Predictions Over Time',fontsize=18)\n",
        "plt.xlabel('Date',fontsize=18)\n",
        "plt.ylabel('Mid Price',fontsize=18)\n",
        "plt.xlim(11000,12500)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}